{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602ca97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import itertools\n",
    "import random\n",
    "import math\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import pickle\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "from functools import reduce\n",
    "import gc\n",
    "\n",
    "ccs=0.1\n",
    "path='gnutella'\n",
    "datasets=[]\n",
    "f=open(\"./dataset/\"+path+'/'+path+\".txt\")\n",
    "data=f.read()\n",
    "f.close()\n",
    "rows=data.split('\\n')\n",
    "for row in rows:\n",
    "    split_row=row.split(' ')\n",
    "    name=(int(split_row[0]),int(split_row[1]),0.01)\n",
    "    datasets.append(name)\n",
    "\n",
    "G=nx.DiGraph()\n",
    "G.add_weighted_edges_from(datasets)   #根据数据集创建有向图\n",
    "\n",
    "del data,rows,datasets\n",
    "gc.collect()\n",
    "\n",
    "node_list=list(G.nodes)\n",
    "nodenum=sorted(node_list,reverse=True)[0]+1\n",
    "\n",
    "total_nodes=len(G.nodes)#总节点数\n",
    "total_edges=len(G.edges)#总边数\n",
    "print('total_nodes:',total_nodes, 'total_edges:',total_edges)\n",
    "community_dict = {}\n",
    "community_set=set()\n",
    "with open(\"./dataset/\"+path+'/'+path+\"_mem.txt\", 'r') as f:\n",
    "    for line in f:\n",
    "        data = line.strip().split()\n",
    "        node_id = int(data[0])  # 节点 ID\n",
    "        group_ids = data[1:]  # 节点 ID 列表 社区信息以str字符串组成的数组组成\n",
    "        community_dict[node_id] = group_ids  # 将节点加入对应的社区\n",
    "\n",
    "# 更新节点的社区信息\n",
    "for node, community_ids in community_dict.items():\n",
    "    if G.has_node(node):  # 检查节点是否存在于图中\n",
    "        G.nodes[node]['community'] = community_ids  # 更新社区信息\n",
    "        community_set.update(community_ids)\n",
    "for u, v, data in G.edges(data=True):\n",
    "    G[u][v]['b_uv']=G[u][v]['weight']\n",
    "    if len(G.nodes[v]['community'])==0: G[u][v]['q_uv']=0\n",
    "    else:G[u][v]['q_uv']=ccs*(1-G[u][v]['b_uv'])/len(G.nodes[v]['community'])\n",
    "community_list=list(community_set)\n",
    "print('community_list len:',len(community_list))\n",
    "\n",
    "def RRS(g=G,t=node_list):# rr-seed\n",
    "    seed = random.choice(t)\n",
    "    result_list = []\n",
    "    result_list.append(seed)\n",
    "    RRS=[]\n",
    "    RRS.append(seed)\n",
    "    # 保存激活的状态\n",
    "    checked = np.zeros(nodenum,dtype=bool) \n",
    "    for node in result_list:\n",
    "        checked[node] = 1\n",
    "    # 当前节点不为空，进行影响\n",
    "    while len(result_list) != 0:\n",
    "        current_node = result_list.pop(0)\n",
    "        for nbr in g.predecessors(current_node): # 得到当前节点的邻居节点\n",
    "            if checked[nbr] == 0:\n",
    "                wt = g.get_edge_data(nbr,current_node)\n",
    "                if random.uniform(0,1) < wt['weight'] :\n",
    "                    result_list.append(nbr)\n",
    "                    checked[nbr] = 1\n",
    "                    RRS.append(nbr)\n",
    "    return RRS\n",
    "\n",
    "def generate_1_hop_vertex_cover(G):\n",
    "    # 创建图的副本，避免修改原图\n",
    "    graph = G.copy()\n",
    "    vertex_cover = set()\n",
    "    \n",
    "    # 当图中还有边时继续迭代\n",
    "    while graph.edges():\n",
    "        # 随机选择一条边\n",
    "        edge = random.choice(list(graph.edges()))\n",
    "        \n",
    "        # 随机选择边的一个顶点\n",
    "        vertex = random.choice(edge)\n",
    "        \n",
    "        # 将顶点加入覆盖集合\n",
    "        vertex_cover.add(vertex)\n",
    "        \n",
    "        # 从图中删除该顶点及其所有关联边\n",
    "        graph.remove_node(vertex)\n",
    "    \n",
    "    return vertex_cover\n",
    "def generate_2_hop_vertex_cover(G):\n",
    "    def two_hop_neighbors(G, v):\n",
    "        # 获取顶点v的邻居\n",
    "        neighbors_v = set(G.neighbors(v))\n",
    "\n",
    "        # 获取邻居的邻居\n",
    "        two_hop_neigh = set()\n",
    "        for neighbor in neighbors_v:\n",
    "            two_hop_neigh.update(G.neighbors(neighbor))\n",
    "\n",
    "        # 两跳的邻居包括邻居的邻居，排除v自己和v的直接邻居\n",
    "        two_hop_neigh -= neighbors_v\n",
    "        two_hop_neigh.discard(v)  # 确保不包含v本身\n",
    "\n",
    "        return two_hop_neigh\n",
    "    graph=G.copy()\n",
    "    sh = []\n",
    "    flag=1\n",
    "    while(1):\n",
    "        out_degrees = graph.out_degree()\n",
    "        max_node = max(out_degrees, key=lambda x: x[1])  # (节点, 出度)\n",
    "        node=max_node[0]\n",
    "        visited=two_hop_neighbors(graph, node)\n",
    "        if(len(visited)==0):\n",
    "            break\n",
    "        graph.remove_node(node)\n",
    "        sh.append(node)\n",
    "        sh.extend(visited)\n",
    "        graph.remove_nodes_from(visited)\n",
    "    print(len(graph.nodes()),len(graph.edges()))\n",
    "    return sh\n",
    "\n",
    "def update_weight(g,F):\n",
    "    def calculate_h_uv(Fv, F, b_uv, q_uv):# 计算 h_uv(Fv, F)\n",
    "        overlap = len(set(Fv) & set(F))  # 计算 Fv 和 F 的交集大小\n",
    "        if(q_uv==0): max_increase=0\n",
    "        else:max_increase = (1 - b_uv) / q_uv  # 根据公式1限制激活概率的上限\n",
    "        return min(max_increase, overlap)\n",
    "    G=g.copy()\n",
    "    # 更新图中每条边的激活概率\n",
    "    for u, v, data in G.edges(data=True):\n",
    "        b_uv = data['b_uv']  # 获取基础激活概率\n",
    "        q_uv = data['q_uv']  # 获取边的增益系数\n",
    "        F_v = G.nodes[v]['community']  # 获取目标节点 v 的社区集合\n",
    "        # 根据公式计算新的激活概率\n",
    "        h_uv_value = calculate_h_uv(F_v, F, b_uv, q_uv)\n",
    "        new_p_uv = b_uv + q_uv * h_uv_value\n",
    "        # 更新边的激活概率\n",
    "        if(new_p_uv!=G[u][v]['weight']):\n",
    "            G[u][v]['weight'] = new_p_uv\n",
    "    return G\n",
    "\n",
    "def find_two_hop_paths_with_max_probability(graph, start): #p1 \n",
    "    # 存储每个目标节点的前两条最大概率路径\n",
    "    paths = defaultdict(list)\n",
    "    \n",
    "    # 深度优先搜索DFS来查找两跳路径\n",
    "    def dfs(node, path, prob, depth):\n",
    "        if depth == 2:\n",
    "            target = path[-1]\n",
    "            # 如果两跳路径的目标已经存在，则将新路径插入\n",
    "            paths[target].append((path, prob))\n",
    "            return\n",
    "        \n",
    "        if node in graph:\n",
    "            for neighbor in graph.successors(node):\n",
    "                # 获取当前边的权重（概率）\n",
    "                edge_prob = graph[node][neighbor]['weight']\n",
    "                dfs(neighbor, path + [neighbor], prob * edge_prob, depth + 1)\n",
    "    \n",
    "    # 查找直接的两跳路径\n",
    "    for target in graph.successors(start):\n",
    "        prob = graph[start][target]['weight']\n",
    "        paths[target].append(([start, target], prob))\n",
    "    \n",
    "    # 开始DFS遍历图\n",
    "    dfs(start, [start], 1, 0)\n",
    "    \n",
    "    # 对每个目标节点按概率排序，并仅保留概率最大的两条路径\n",
    "    result = {}\n",
    "    for target in paths:\n",
    "        # 按照路径的总概率排序\n",
    "        sorted_paths = sorted(paths[target], key=lambda x: x[1], reverse=True)\n",
    "        result[target] = sorted_paths[:2]  # 取概率最大的两条路径\n",
    "    \n",
    "    return result\n",
    "def find_reverse_two_hop_paths_with_max_probability(graph, start):\n",
    "    # 创建反向图\n",
    "    reverse_graph = graph.reverse()  # 反向图\n",
    "    \n",
    "    # 存储每个目标节点的前两条最大概率路径\n",
    "    paths = defaultdict(list)\n",
    "    \n",
    "    # 深度优先搜索DFS来查找反向两跳路径\n",
    "    def dfs(node, path, prob, depth):\n",
    "        if depth == 2:\n",
    "            target = path[-1]\n",
    "            # 如果反向两跳路径的目标已经存在，则将新路径插入\n",
    "            paths[target].append((path, prob))\n",
    "            return\n",
    "        \n",
    "        if node in reverse_graph:\n",
    "            for neighbor in reverse_graph.successors(node):\n",
    "                # 获取当前边的权重（概率）\n",
    "                edge_prob = reverse_graph[node][neighbor]['weight']  # 默认权重为1\n",
    "                dfs(neighbor, path + [neighbor], prob * edge_prob, depth + 1)\n",
    "    \n",
    "    # 查找直接的反向两跳路径\n",
    "    for target in reverse_graph.successors(start):\n",
    "        prob = reverse_graph[start][target]['weight']\n",
    "        paths[target].append(([start, target], prob))\n",
    "    \n",
    "    # 开始DFS遍历反向图\n",
    "    dfs(start, [start], 1, 0)\n",
    "    \n",
    "    # 对每个目标节点按概率排序，并仅保留概率最大的两条路径\n",
    "    result = {}\n",
    "    for target in paths:\n",
    "        # 按照路径的总概率排序\n",
    "        sorted_paths = sorted(paths[target], key=lambda x: x[1], reverse=True)\n",
    "        result[target] = sorted_paths[:2]  # 取概率最大的两条路径\n",
    "    \n",
    "    return result\n",
    "\n",
    "def pf_path(result,s,path):\n",
    "    path[s]={}\n",
    "    for target, paths in result.items():\n",
    "        if len(paths)==1: path[s][target]=[paths[0][0]]\n",
    "        else:\n",
    "            path[s][target]=[paths[0][0],paths[1][0]]\n",
    "    return path\n",
    "\n",
    "def get_forward_probability(G,path_list):\n",
    "    if len(path_list)==2:\n",
    "        return G[path_list[0]][path_list[1]]['weight']\n",
    "    elif len(path_list)==3:\n",
    "        return G[path_list[0]][path_list[1]]['weight']*G[path_list[1]][path_list[2]]['weight']\n",
    "    else:\n",
    "        print(path_list,\"get_forward_probability有误！\")\n",
    "        return None\n",
    "\n",
    "def get_reverse_probability(G,path_list):\n",
    "    if len(path_list)==2:\n",
    "        return G[path_list[1]][path_list[0]]['weight']\n",
    "    elif len(path_list)==3:\n",
    "        return G[path_list[2]][path_list[1]]['weight']*G[path_list[1]][path_list[0]]['weight']\n",
    "    else:\n",
    "        print(path_list,\"get_reverse_probability有误！\")\n",
    "        return None\n",
    "\n",
    "#计算IMM\n",
    "def RRS_ori(g=G,t=node_list):#最原始的rr-seed \n",
    "    seed = random.choice(t)\n",
    "    result_list = []\n",
    "    result_list.append(seed)\n",
    "    RRS=[]\n",
    "    RRS.append(seed)\n",
    "    # 保存激活的状态\n",
    "    checked = np.zeros(nodenum,dtype=bool) \n",
    "    for node in result_list:\n",
    "        checked[node] = 1\n",
    "    # 当前节点不为空，进行影响\n",
    "    while len(result_list) != 0:\n",
    "        current_node = result_list.pop(0)\n",
    "        for nbr in g.predecessors(current_node): # 得到当前节点的邻居节点\n",
    "            if checked[nbr] == 0:\n",
    "                wt = g.get_edge_data(nbr,current_node)\n",
    "                if random.uniform(0,1) < wt['weight'] :\n",
    "                    result_list.append(nbr)\n",
    "                    checked[nbr] = 1\n",
    "                    RRS.append(nbr)\n",
    "    return RRS\n",
    "def combination(n, k):\n",
    "    if k < 0 or k > n:\n",
    "        return 0\n",
    "    # 利用对称性，C(n, k) == C(n, n-k)\n",
    "    if k > n - k:\n",
    "        k = n - k\n",
    "    result = 1\n",
    "    for i in range(1, k + 1):\n",
    "        result = result * (n - i + 1) // i\n",
    "    return result\n",
    "def lnc(n,k):\n",
    "    return math.log(combination(n, k))\n",
    "def nodeselect(RR,k):#对RRSet作种子选择\n",
    "    s=time.time()\n",
    "    R=RR.copy()\n",
    "    js=0\n",
    "    SEED=[]\n",
    "    for _ in range(k):\n",
    "        flat_map = [item for subset in R for item in subset]\n",
    "        if(len(flat_map))==0:\n",
    "            return SEED,0\n",
    "        seed = Counter(flat_map).most_common()[0][0]\n",
    "        js=js+Counter(flat_map).most_common()[0][1]\n",
    "        SEED.append(seed)\n",
    "        R = [rrs for rrs in R if seed not in rrs]\n",
    "    return SEED,js\n",
    "def IMM_RRS(G,k,eps,ell,node_list=node_list,n=total_nodes):\n",
    "    s = time.time()\n",
    "    # 参数计算（保持不变）\n",
    "    ell=ell*(1+math.log(2)/math.log(n))\n",
    "    eps1=math.sqrt(2) * eps\n",
    "    alpha = math.sqrt(ell * math.log(n) + math.log(2))\n",
    "    beta = math.sqrt((1.0 - 1.0 / math.exp(1)) * (lnc(n,k) + alpha * alpha))\n",
    "    lamba1 = (2*n*(1+1/3*eps1) * (lnc(n,k)+ell*math.log(n)+math.log(math.log(n)/math.log(2))))/(eps1*eps1)\n",
    "    lamba2 = 2 * n * pow(((1.0 - 1.0 / math.exp(1)) * alpha + beta),2) / (eps*eps)\n",
    "    LB=1\n",
    "    # 初始化映射（b直接存储RR集，a和cover_counts实时更新）\n",
    "    a = defaultdict(list)  # a[node] = [rrset_idx1, ...]\n",
    "    b = []  # b[rr_idx] = [node1, ...]（替代原R）\n",
    "    cover_counts = defaultdict(int)  # 节点覆盖计数\n",
    "    \n",
    "    len_b=0\n",
    "    # 校准阶段\n",
    "    for i in range(1, int(math.log(n-1) / math.log(2)) + 1):\n",
    "        x = n / pow(2, i)\n",
    "        ti = int(lamba1 / x)\n",
    "        # 生成RR集并实时更新映射\n",
    "        \n",
    "        while len_b < ti:\n",
    "            rrset = RRS_ori(G, node_list)\n",
    "            b.append(rrset)\n",
    "            for node in rrset:\n",
    "                a[node].append(len_b)\n",
    "                cover_counts[node] += 1\n",
    "            len_b=len(b)\n",
    "        \n",
    "        # 无需复制，直接传递原始映射（函数内部不会修改它们）\n",
    "        remaining_rrs = set(range(len(b)))\n",
    "        SEED, js = optimized_nodeselect(k, a, b, cover_counts, remaining_rrs)\n",
    "        \n",
    "        if n * js / len(b) >= (1 + eps1) * x:\n",
    "            LB = n * js / ((1 + eps1) * len(b))\n",
    "            break\n",
    "    \n",
    "    # 最终采样阶段\n",
    "    theta = int(lamba2 / LB)\n",
    "    print(f\"len(RR): {theta}\")\n",
    "    \n",
    "    # 继续生成RR集至目标数量\n",
    "    \n",
    "    while len_b < theta:\n",
    "        rrset = RRS_ori(G, node_list)\n",
    "        b.append(rrset)\n",
    "        for node in rrset:\n",
    "            a[node].append(len_b)\n",
    "            cover_counts[node] += 1\n",
    "        len_b=len(b)\n",
    "    \n",
    "    # 最终种子选择（同样无需复制）\n",
    "    remaining_rrs = set(range(len(b)))\n",
    "    seed, js = optimized_nodeselect(k, a, b, cover_counts, remaining_rrs)\n",
    "    \n",
    "    print(f\"IMM_RRS COST TIME: {time.time() - s:.2f}s\")\n",
    "    return seed\n",
    "\n",
    "def optimized_nodeselect(k, a, b, cover_counts, remaining_rrs):\n",
    "    \"\"\"无需复制原始映射，仅在内部维护临时状态\"\"\"\n",
    "    s = time.time()\n",
    "    SEED = []\n",
    "    total_covered = 0\n",
    "    \n",
    "    # 仅复制需要修改的状态（覆盖计数和剩余RR集）\n",
    "    current_cover = cover_counts.copy()  # 复制需要修改的计数\n",
    "    current_remaining = remaining_rrs.copy()  # 复制需要修改的集合\n",
    "    \n",
    "    for _ in range(k):\n",
    "        if not current_remaining or not current_cover:\n",
    "            break\n",
    "        \n",
    "        # 选择覆盖最多的节点\n",
    "        seed = max(current_cover, key=lambda x: current_cover[x])\n",
    "        SEED.append(seed)\n",
    "        \n",
    "        # 找到覆盖的RR集\n",
    "        covered_rrs = [rr_idx for rr_idx in a[seed] if rr_idx in current_remaining]\n",
    "        total_covered += len(covered_rrs)\n",
    "        \n",
    "        # 更新剩余RR集\n",
    "        for rr_idx in covered_rrs:\n",
    "            current_remaining.discard(rr_idx)\n",
    "        \n",
    "        # 更新其他节点的覆盖计数\n",
    "        for rr_idx in covered_rrs:\n",
    "            for node in b[rr_idx]:\n",
    "                if node != seed and node in current_cover:\n",
    "                    current_cover[node] -= 1\n",
    "                    if current_cover[node] == 0:\n",
    "                        del current_cover[node]\n",
    "        \n",
    "        # 移除已选节点\n",
    "        del current_cover[seed]\n",
    "    \n",
    "    #print(f\"节点选择耗时: {time.time() - s:.2f}秒\")\n",
    "    return SEED, total_covered\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a47f09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_two_hop_probability(graph, start_node):\n",
    "    hop_probabilities = {}\n",
    "    \n",
    "    # 计算第一跳的激活概率\n",
    "    first_hop_probs = {}\n",
    "    for neighbor in graph.successors(start_node):  # 只考虑a的邻居\n",
    "        # 计算第一跳传播概率\n",
    "        hop_probabilities[neighbor] = graph[start_node][neighbor]['weight']  # 保存第一跳的概率\n",
    "    # 计算第二跳的激活概率\n",
    "    for node in graph.successors(start_node):\n",
    "        for second_node in graph.successors(node):  # 第二跳从node的邻居开始\n",
    "            # 计算第二跳的传播概率\n",
    "            if(second_node not in hop_probabilities.keys()):hop_probabilities[second_node]=graph[node][second_node]['weight']*graph[start_node][node]['weight']\n",
    "            else:\n",
    "                hop_probabilities[second_node]=1-(1-hop_probabilities[second_node])*(1-graph[node][second_node]['weight']*graph[start_node][node]['weight']) \n",
    "    return hop_probabilities\n",
    "# t=time.time()\n",
    "# sh=generate_1_hop_vertex_cover(G)\n",
    "# print('generate_1_hop_vertex_cover cost time:',time.time()-t)\n",
    "# with open(\"./dataset/\"+path+'/'+\"sh.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(sh, f)  # 将数据序列化后写入文件\n",
    "with open(\"./dataset/\"+path+'/'+\"sh.pkl\", \"rb\") as f:\n",
    "    sh=pickle.load(f)\n",
    "\n",
    "t=time.time()\n",
    "graph={}\n",
    "for i in community_list:\n",
    "    graph[i]=update_weight(G,i)\n",
    "    \n",
    "print('gragh:',time.time()-t)\n",
    "\n",
    "t=time.time()\n",
    "table={}\n",
    "\n",
    "for node in sh:\n",
    "    table[node]={}\n",
    "    G_probability={}\n",
    "    G_2hop=calculate_two_hop_probability(G, node)\n",
    "    temp_dict={}\n",
    "    for i in community_list:\n",
    "        G_probability[i]=calculate_two_hop_probability(graph[i], node)\n",
    "    for key,value in G_2hop.items():\n",
    "        x0=G_2hop[key]\n",
    "        for i in community_list:\n",
    "            temp_dict[i]=G_probability[i][key]\n",
    "        table[node][key]=temp_dict\n",
    "\n",
    "with open(\"./dataset/\"+path+'/'+\"table.pkl\", \"wb\") as f:\n",
    "    pickle.dump(table, f)  # 将数据序列化后写入文件\n",
    "print('table cost time:',time.time()-t)\n",
    "print('len(sh):',len(sh))\n",
    "\n",
    "num=0\n",
    "for node in sh:\n",
    "    num=num+len(table[node])\n",
    "print('len(If[node])+len(Ir[node])',num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab282898",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_p_weight(F,x0,temp):\n",
    "    X=[]\n",
    "    for i in F:\n",
    "        X.append(temp[i])\n",
    "    if x0==1:\n",
    "        return 1\n",
    "    else:\n",
    "        m = len(X) - 1  # 这里假设 X 里有 m+1 个元素，X[0] 是 X_0，X[m] 是 X_m\n",
    "        sum_part1 = sum(1 - X[j] for j in range(m))  # 计算第一部分的和\n",
    "        sum_part2 = 0\n",
    "        for i in range(m):\n",
    "            for j in range(i+1, m):\n",
    "                sum_part2 += (1 - x0) + (1 - X[i]) * (1 - X[j]) / (1 - x0)  # 计算第二部分的和\n",
    "        part3 = (m - 1) * math.sqrt(1 - x0)  # 第三部分\n",
    "        result = 1 - (math.sqrt(sum_part1 + sum_part2) - part3) ** 2  # 最终计算结果\n",
    "        return max(result,max(X),1)\n",
    "def update_index_table(table,F,sh):\n",
    "    t=time.time()\n",
    "    new_forward_label={}\n",
    "    for node in sh:\n",
    "        new_forward_label[node]={}\n",
    "        for key,value in table[node].items():\n",
    "            x0=value[0]\n",
    "            temp_dict=value[1]\n",
    "            new_forward_label[node][key]=update_p_weight(F,x0,temp_dict)\n",
    "    \n",
    "    print(f\"new_forward_label cost time:{time.time()-t}\")\n",
    "    return new_forward_label\n",
    "\n",
    "def reverse_dict(Ir):\n",
    "    I = {}\n",
    "    # 遍历 Ir 字典\n",
    "    for key1, value1 in Ir.items():\n",
    "        for key2, value2 in value1.items():\n",
    "            if key2 not in I:\n",
    "                I[key2] = {}\n",
    "            I[key2][key1] = value2\n",
    "    return I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b3cb2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_single(seed,g):#理论上不需要层级\n",
    "    #s=time.time()\n",
    "    result = []\n",
    "    result.extend(seed)\n",
    "    # 保存激活的状态\n",
    "    checked_a = np.zeros(nodenum,dtype=bool)\n",
    "    for t in result:\n",
    "        checked_a[t] = 1\n",
    "    # 当前节点不为空，进行影响\n",
    "    while(len(result)>0):\n",
    "        current_node = result.pop(0)\n",
    "        for nbr in g.successors(current_node): # 得到当前节点的邻居节点\n",
    "            if checked_a[nbr] == 0 :\n",
    "                wt = g.get_edge_data(current_node,nbr)\n",
    "                if random.uniform(0,1)<wt['weight'] :\n",
    "                    result.append(nbr)\n",
    "                    checked_a[nbr] = 1\n",
    "    #print('forward:',time.time()-s)\n",
    "    return (checked_a != 0).sum()\n",
    "def pg_influence(G,k,seed_set):#转为列表存储，可以评估方差\n",
    "    sss=0\n",
    "    for i in range(k):\n",
    "        sss+=forward_single(seed_set,G)\n",
    "    print(\"influence:\",sss/k)\n",
    "    return sss/k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db4a2f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_labels(new_forward_label):\n",
    "    t=time.time()\n",
    "    best_for_b = {}\n",
    "    pruned = {}\n",
    "    for a, b_dict in new_forward_label.items():\n",
    "        pruned[a]={}\n",
    "        for b, value in b_dict.items():\n",
    "            # 如果 b 尚未出现，或当前 value 更大，则更新\n",
    "            if b not in best_for_b or value > best_for_b[b][1]:\n",
    "                best_for_b[b] = (a, value)\n",
    "    for b, (a, value) in best_for_b.items():\n",
    "        pruned[a][b] = value\n",
    "    for key,temp in pruned.items():\n",
    "        pruned[key]=sum(temp.values())+1\n",
    "    print(f\"jz cost time:{time.time()-t}\")\n",
    "    return pruned\n",
    "\n",
    "def new_table(new_forward_label):\n",
    "    t=time.time()\n",
    "    yxl_table = {}\n",
    "    for a, b_dict in new_forward_label.items():\n",
    "        yxl_table[a]=1\n",
    "        for b, value in b_dict.items():\n",
    "            yxl_table[a]+=value\n",
    "    print(f\"new_table cost time:{time.time()-t}\")\n",
    "    return yxl_table\n",
    "\n",
    "def update_node_influence1(G,seed_set,yxl_table,new_forward_label):#需要再好好考虑具体计算影响力增益算法\n",
    "    def get_activation_probabilities(graph, seed_set):\n",
    "\n",
    "        # 步骤 1: 找到所有不属于S的一跳邻居\n",
    "        one_hop_neighbors = set()\n",
    "        for node in seed_set:\n",
    "            # graph.neighbors(node) 可以获取所有邻居（对于有向图和无向图都适用）\n",
    "            one_hop_neighbors.update(graph.neighbors(node))\n",
    "        \n",
    "        # 从邻居集合中排除种子集自身\n",
    "        one_hop_neighbors -= seed_set\n",
    "\n",
    "        # 步骤 2: 为每个邻居计算被激活的总概率\n",
    "        activation_probs = {}\n",
    "        for neighbor in one_hop_neighbors:\n",
    "            # 对于每个邻居，计算所有从S到它的边都不激活的概率\n",
    "            prob_not_activated = 1.0\n",
    "            \n",
    "            # 遍历种子集，查找连接到当前邻居的边\n",
    "            for s_node in seed_set:\n",
    "                # 检查是否存在从 s_node到neighbor的边\n",
    "                if graph.has_edge(s_node, neighbor):\n",
    "                    # 获取边的激活概率，如果不存在'probability'属性，则默认为0\n",
    "                    edge_prob = graph[s_node][neighbor].get('weight', 0.0)\n",
    "                    prob_not_activated *= (1 - edge_prob)\n",
    "                \n",
    "            # 总的激活概率 = 1 - (所有相关边都不激活的概率)\n",
    "            total_activation_prob = 1 - prob_not_activated\n",
    "            activation_probs[neighbor] = total_activation_prob\n",
    "            \n",
    "        return activation_probs\n",
    "    indexnode=set()\n",
    "    for seedNode in seed_set:\n",
    "        if seedNode in new_forward_label.keys():indexnode.update(seedNode)\n",
    "    others=set(seed_set)-indexnode\n",
    "    activation_probs=get_activation_probabilities(G,others)\n",
    "\n",
    "    tuple_list=[]\n",
    "    p=1\n",
    "    temp=0\n",
    "    for node in indexnode:\n",
    "        tuple_list.append((1,yxl_table[nbr]))\n",
    "    for nbr in activation_probs.keys(): # 得到当前节点的邻居节点\n",
    "        value = activation_probs[nbr]\n",
    "        tuple_list.append((value,value*yxl_table[nbr]))\n",
    "    # 根据value*yxl_table[nbr]进行排序\n",
    "    tuple_list.sort(key=lambda t: t[1], reverse=True)\n",
    "    \n",
    "    for t in tuple_list:\n",
    "        temp+=t[1]*p\n",
    "        p=p*(1-t[0])\n",
    "    return temp\n",
    "\n",
    "def update_node_influence2(G,node,pruned,new_forward_label):#用剪枝表继续计算影响力\n",
    "    if node in new_forward_label.keys():\n",
    "        temp=pruned[node]\n",
    "    else:\n",
    "        temp=1\n",
    "        for nbr in G.successors(node): # 得到当前节点的邻居节点\n",
    "            value = G.get_edge_data(node,nbr)['weight']\n",
    "            if nbr in pruned.keys():\n",
    "                temp+=value*pruned[nbr]\n",
    "            else:\n",
    "                temp+=value\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb3c863",
   "metadata": {},
   "outputs": [],
   "source": [
    "path='gnutella'\n",
    "def count_communities(path='gnutella'):\n",
    "    # 用于存储每个社区的节点数量\n",
    "    community_counts = {}\n",
    "    file_path=\"./dataset/\"+path+'/'+path+\"_mem.txt\"\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            # 去除行首尾的空白字符\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "                \n",
    "            # 分割行数据，取前两个数字作为节点编号和社区号\n",
    "            parts = line.split()\n",
    "            if len(parts) >= 2:\n",
    "                try:\n",
    "                    # 节点编号我们不需要实际使用，只需要社区号\n",
    "                    community_id = int(parts[1])\n",
    "                    \n",
    "                    # 统计社区数量\n",
    "                    if community_id in community_counts:\n",
    "                        community_counts[community_id] += 1\n",
    "                    else:\n",
    "                        community_counts[community_id] = 1\n",
    "                except ValueError:\n",
    "                    # 忽略格式不正确的行\n",
    "                    continue\n",
    "    \n",
    "    # 按社区节点数量从大到小排序\n",
    "    sorted_communities = sorted(community_counts.items(), \n",
    "                               key=lambda x: x[1], \n",
    "                               reverse=True)\n",
    "    community_list=[]\n",
    "    # 取前10个，或者如果总社区数不足10个则取全部\n",
    "    top_count = 10\n",
    "    for i in range(top_count):\n",
    "        community_id, count = sorted_communities[i]\n",
    "        community_list.append(community_id)\n",
    "    return community_list\n",
    "F=count_communities(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6953d319",
   "metadata": {},
   "outputs": [],
   "source": [
    "newG=update_weight(G,F)\n",
    "t2=time.time()\n",
    "new_forward_label=update_index_table(table,F,sh)\n",
    "tt1=time.time()-t2\n",
    "pruned=prune_labels(new_forward_label)\n",
    "tt2=time.time()-t2-tt1\n",
    "\n",
    "tt=time.time()\n",
    "yxl_table=new_table(new_forward_label)\n",
    "tt3=time.time()-tt\n",
    "\n",
    "print('new_forward_label',tt1)\n",
    "print('prune_labels',tt2)\n",
    "print('yxl_table',tt3)\n",
    "\n",
    "for k in [20,30,50,100,150,200]:\n",
    "    print(k,'#########################################################################')\n",
    "    repeat=100\n",
    "    # t1=time.time()\n",
    "    # seed4=IMM_RRS(newG,k,0.5,1)\n",
    "    # print('time:',time.time()-t1,'IMM influence:')\n",
    "    # pg_influence(newG,repeat,seed4)\n",
    "    t2=time.time()\n",
    "    new_sorted_list=[]\n",
    "    for i in range(k):\n",
    "        node=sorted_list[i][0]\n",
    "        yxl=update_node_influence(newG,node,new_forward_label)\n",
    "        new_sorted_list.append((node,yxl))\n",
    "    new_sorted_list = sorted(new_sorted_list, key=lambda x: x[1], reverse=True)\n",
    "    temp=0\n",
    "    for i in range(k,5*k):\n",
    "        node=sorted_list[i][0]\n",
    "        \n",
    "        yxl=update_node_influence(newG,node,new_forward_label)\n",
    "        if(yxl>new_sorted_list[k-1][1]):\n",
    "            temp=0\n",
    "            new_tuple=(node,yxl)\n",
    "            new_sorted_list=insert_list(new_tuple,new_sorted_list,k)\n",
    "        else:\n",
    "            temp=temp+1\n",
    "            if(temp>=k):\n",
    "                print(i)\n",
    "                break\n",
    "    seed3=[item[0] for item in new_sorted_list[:k]]\n",
    "    print('time:',time.time()-t2+tt1,'stop round:',i,'my method1 influence:')\n",
    "    pg_influence(newG,repeat,seed3)\n",
    "    \n",
    "    \n",
    "    t3=time.time()\n",
    "    new_sorted_list=[]\n",
    "    for i in range(k):\n",
    "        node=sorted_list[i][0]\n",
    "        yxl=update_node_influence2(newG,node,pruned,new_forward_label)\n",
    "        new_sorted_list.append((node,yxl))\n",
    "    new_sorted_list = sorted(new_sorted_list, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    temp=0\n",
    "    for i in range(k,10*k):\n",
    "        node=sorted_list[i][0]\n",
    "        yxl=update_node_influence2(newG,node,pruned,new_forward_label)\n",
    "        if(yxl>new_sorted_list[k-1][1]):\n",
    "            temp=0\n",
    "            new_tuple=(node,yxl)\n",
    "            new_sorted_list=insert_list(new_tuple,new_sorted_list,k)\n",
    "        else:\n",
    "            temp=temp+1\n",
    "            if(temp>=2*k):\n",
    "                break\n",
    "    seed3=[item[0] for item in new_sorted_list[:k]]\n",
    "    print('time:',time.time()-t3+tt1+tt2,'stop round:',i,'my method2 influence:')\n",
    "    pg_influence(newG,repeat,seed3)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
